{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Create the conda environment (gcmeta) for running the scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. install anaconda\n",
    "2. conda create --name gcmeta --file gcmeta_requirement.txt\n",
    "3. conda activate gcmeta\n",
    "4. pip install pymassspec\n",
    "5. pip install pyms-nist-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Make a copy of required files to a directory (not required to be in the same directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. plt_8pt.mplstyle (for plotting styles)\n",
    "2. gc_analysis.py (if running all functions from this file, easier)\n",
    "3. gcms screening results (in CDF format): including core-only, core_CYPs, and 2 alkane standards\n",
    "4. example screening results for this study can be found at: https://zenodo.org/record/7703726"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3. Provide required information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_id = 'LRD01'                                ### LRD scaffold name\n",
    "cdf_path = './LRD01_screening/'                  ### folder for cdf files\n",
    "module_path = 'C:/module/'                       ### folder for gc_analysis.py\n",
    "nist_path = 'C:/NIST17/MSSEARCH/mainlib'         ### path for the NIST mainlib\n",
    "style_path = './plt_8pt.mplstyle'                ### path for mplstyle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Preamble (option 1: importing functions from gc_analysis.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from gc_analysis import *\n",
    "\n",
    "search = pyms_nist_search.Engine(nist_path,\n",
    "                                 pyms_nist_search.NISTMS_MAIN_LIB,\n",
    "                                 './')\n",
    "plt.style.use(style_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preamble (option 2: running all scripts in the notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 0. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "#import pickle5 as pickle ## original: import pickle\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "from scipy import spatial\n",
    "\n",
    "from pyms.BillerBiemann import BillerBiemann, num_ions_threshold, rel_threshold\n",
    "from pyms.Experiment import Experiment\n",
    "from pyms.GCMS.IO.ANDI import ANDI_reader\n",
    "from pyms.IntensityMatrix import build_intensity_matrix_i\n",
    "from pyms.Noise.SavitzkyGolay import savitzky_golay\n",
    "from pyms.Noise.Analysis import window_analyzer\n",
    "from pyms.Peak.Function import peak_sum_area, peak_top_ion_areas\n",
    "from pyms.TopHat import tophat\n",
    "import matplotlib.pyplot as plt\n",
    "from pyms.Display import plot_ic, plot_peaks\n",
    "from pyms.DPA.PairwiseAlignment import PairwiseAlignment, align_with_tree\n",
    "from pyms.DPA.Alignment import exprl2alignment\n",
    "from pyms.Experiment import load_expr\n",
    "from pyms.Spectrum import normalize_mass_spec\n",
    "from pyms.Peak.List.IO import load_peaks, store_peaks\n",
    "\n",
    "import pyms_nist_search\n",
    "import pubchempy as pcp\n",
    "\n",
    "import rdkit as Chem\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.platypus import BaseDocTemplate, PageTemplate, Flowable\n",
    "from reportlab.platypus import Table, TableStyle, Spacer, Paragraph, Image, Frame, FrameBreak\n",
    "from reportlab.lib.units import inch\n",
    "\n",
    "from pdfrw import PdfReader, PdfDict\n",
    "from pdfrw.buildxobj import pagexobj\n",
    "from pdfrw.toreportlab import makerl\n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "## convert svg to pdf, new\n",
    "from svglib.svglib import svg2rlg\n",
    "from reportlab.graphics import renderPDF\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "Draw.DrawingOptions.bondLineWidth = 2\n",
    "Draw.DrawingOptions.atomLabelFontSize = 20\n",
    "from rdkit.Chem.Descriptors import ExactMolWt\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "search = pyms_nist_search.Engine(nist_path,\n",
    "                                 pyms_nist_search.NISTMS_MAIN_LIB,\n",
    "                                 './')\n",
    "plt.style.use(style_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1. Functions for initial data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def export_ic(ic, filename, outpath, mass = None):\n",
    "    '''\n",
    "    export ion chromatogram to csv file\n",
    "    \n",
    "    ic: ion chromatogram object\n",
    "    name: string, output file name\n",
    "    folder: string, output folder\n",
    "    mass: only for EIC\n",
    "    '''\n",
    "    \n",
    "    if not os.path.exists(outpath):\n",
    "        os.makedirs(outpath)\n",
    "    \n",
    "    if mass == None:\n",
    "        print(f'\\t export TIC of {filename} to {outpath}TIC_{filename}.csv')\n",
    "    else:\n",
    "        print(f'\\t export EIC of {filename} at m/z {mass} to {outpath}EIC_{mass}_{filename}.csv')\n",
    "        \n",
    "    chromatogram = {'time': [float(i)/60 for i in ic.time_list],\n",
    "                    'counts': ic.intensity_array_list}\n",
    "    pd.DataFrame(chromatogram).to_csv(f'{outpath}/{filename}.csv', index = False)    \n",
    "    \n",
    "def process_cdf(cdf_file,name):\n",
    "    '''\n",
    "    input: raw cdf file directly output from chemstation\n",
    "    output TIC and leco csv of the input data     \n",
    "    '''\n",
    "\n",
    "    # name = re.search(r'.*/(.*)/.CDF', cdf_file)[1]\n",
    "    print(f'Processing experiment \"{name}\"')\n",
    "    \n",
    "    # read cdf file and build intensity matrix\n",
    "    # time by mass matrix containing signal intensity at each time point (in seconds) and at each m/z\n",
    "    data = ANDI_reader(cdf_file)\n",
    "    im = build_intensity_matrix_i(data)  \n",
    "    tic = im.tic\n",
    "    export_ic(tic, name, './tic/')\n",
    "    \n",
    "    return im\n",
    "    \n",
    "def smooth_im(im, name, ref_rt):\n",
    "    '''smooth chromatogram and baseline correction\n",
    "       ref_rt: string of retention time for the structural element\n",
    "    '''\n",
    "    \n",
    "    print(f'\\t smoothing chromatogram of {name}')\n",
    "    \n",
    "    n_scan, n_mz = im.size\n",
    "    \n",
    "    # Preprocess the data (Savitzky-Golay smoothing and Tophat baseline detection)\n",
    "    for ii in range(n_mz):\n",
    "        ic = im.get_ic_at_index(ii)  # (extracted) ion chromatogram\n",
    "        ic_smooth = savitzky_golay(ic)  # smooth (extracted) ion chromatogram\n",
    "        ic_bc = tophat(ic_smooth, struct = ref_rt)  # baseline correction, providing a reference for correction\n",
    "        im.set_ic_at_index(ii, ic_bc)  # update the intensity matrix\n",
    "        \n",
    "    return im\n",
    "\n",
    "def peak_picking(im, msrange, ignorems = (0), minnoise = 600,\n",
    "                 BBpoints = 9, BBscans = 2, threshold = 2, numsignal = 7):\n",
    "    ''' peak detection, return a list of peak objects\n",
    "    \n",
    "        im: intensitry matrix input\n",
    "        msrange: list of int, use to crop the ms of peak objects\n",
    "        ignorems: tuple of int, m/z to be ignored, removed in peak objects\n",
    "        BBpoints: int, number of scans to be considered as local maxima\n",
    "        BBscans: int, number of scans, for which ions are combined  \n",
    "        threshold: int, ion intensity threshold in % to be kept in each peak object\n",
    "        numsignal: int, # of ions above noise level in a peak object to determine whether it is kept\n",
    "    '''\n",
    "    tic = im.tic\n",
    "    \n",
    "    # peak (species) detection\n",
    "    # return a list of peak objects\n",
    "    pl = BillerBiemann(im, points = BBpoints, scans = BBscans)\n",
    "\n",
    "    # remove low intensity siganls (m/z, 2%) in each peak (species)\n",
    "    apl = rel_threshold(pl, percent = threshold)\n",
    "\n",
    "    # Trim the peak list by noise threshold (automatically determined by window_analyzer or provide a fix number)\n",
    "    # the peak (species) must have at leat n signals (m/z) larger than the threshold\n",
    "    noise_level = window_analyzer(tic)\n",
    "    peak_list = num_ions_threshold(apl, n = numsignal, cutoff = max(noise_level, minnoise))\n",
    "    \n",
    "    print(f'\\t Number of Peaks found: {len(peak_list)}')\n",
    "    \n",
    "    for peak in peak_list:\n",
    "        peak.crop_mass(msrange[0], msrange[1])\n",
    "        \n",
    "        # siloxanes (common background in GC-MS)\n",
    "        for mass in ignorems:\n",
    "            peak.null_mass(mass)\n",
    "        \n",
    "        # peak area integrated over all signals (all m/z)\n",
    "        peak.area = peak_sum_area(im, peak) + 1\n",
    "        \n",
    "        # peak area using only the higheset signal (m/z)\n",
    "        peak.ion_areas = peak_top_ion_areas(im, peak)\n",
    "        \n",
    "    return(peak_list)\n",
    "\n",
    "def create_exp(name, peak_list, rtrange = ['4.5m', '13m']):\n",
    "    '''create and save and return experiment objects\n",
    "       name: str, identifier\n",
    "       peak_list: list of peak objects\n",
    "       rtrange: list of str in minutes, only keep peaks within this range\n",
    "    '''\n",
    "    if not os.path.exists('experiment'):\n",
    "        os.makedirs('experiment')\n",
    "        \n",
    "    expr = Experiment(name, peak_list)\n",
    "\n",
    "    # Use the same retention time range for all experiments\n",
    "    # (this can be done earlier when loading the data)\n",
    "    expr.sele_rt_range(rtrange)\n",
    "\n",
    "    # Save the experiment to disk for future use\n",
    "    output_file = name + '.expr'\n",
    "    expr.dump('./experiment/' + output_file)\n",
    "    print(f'\\t Number of Peaks saved: {len(expr.peak_list)}')\n",
    "    print(f\"\\t Saving the result as ./experiment/{output_file}\")\n",
    "    \n",
    "    return expr\n",
    "\n",
    "\n",
    "def raw_data_process(core_id, cdf_path, ref_rt,\n",
    "                     msrange = [53, 350], ignorems = (207, 281), minnoise = 500,\n",
    "                     rtrange = ['5m', '12.5m'], test = False):\n",
    "\n",
    "    file_ids_temp = []     # which LRD-CYP combination\n",
    "    num_peaks = []         # total peaks detected\n",
    "    num_peaks_saved = []   # num of peaks within the pre-defined retention time range\n",
    "    \n",
    "    \n",
    "    for file in glob.glob(f'{cdf_path}*.CDF'):\n",
    "        file_ids_temp.append(re.search(r'\\\\(.*)\\.CDF', file)[1])\n",
    "\n",
    "\n",
    "    # pick five files to test how many peaks detected -> if parameters need to be adjusted\n",
    "    # if test == False, all files will be processed\n",
    "    if test:\n",
    "        file_ids = []\n",
    "        for i in range(5):\n",
    "            file_ids.append(random.choice(file_ids_temp))\n",
    "    if not test:\n",
    "        file_ids = file_ids_temp\n",
    "        \n",
    "    for file_id in file_ids:\n",
    "        # create intensity matrix and save TIC to csv file\n",
    "        file = f'{cdf_path}' + file_id + '.CDF'\n",
    "        im = process_cdf(file, file_id)\n",
    "\n",
    "        # smooth chromatogram\n",
    "        sim = smooth_im(im, file_id, ref_rt)\n",
    "\n",
    "        # peak picking and save to expr\n",
    "        peak_list = peak_picking(sim, msrange, ignorems, minnoise)\n",
    "        expr = create_exp(file_id, peak_list, rtrange)\n",
    "\n",
    "        # save info to lists\n",
    "        num_peaks.append(len(peak_list))\n",
    "        num_peaks_saved.append(len(expr.peak_list))\n",
    "    \n",
    "    \n",
    "    # create peak_info.csv if it's not a test run    \n",
    "    if not test:\n",
    "        peak_info = pd.DataFrame({'#peaks': num_peaks,\n",
    "                                  '#peaks_final': num_peaks_saved},\n",
    "                                  index = file_ids)\n",
    "\n",
    "        peak_info.to_csv(core_id + '_peak_info.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2. Functions for peak alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def alignment(expr_path, type = 'exp', Dw = 0.35, Gw = 0.3):\n",
    "    '''\n",
    "    align chromatograms to generate alignment objects\n",
    "    expr_path: str, the folder containing expr files (dumped peak lists)\n",
    "    type: str, exp or std, generating alignment objects for experiments of alkane standards\n",
    "    Dw: float, rt modulation in seconds (allowed drift in retention time)\n",
    "    Gw, float, gap penalty\n",
    "    \n",
    "    dump an alignment object to ./align/\n",
    "    output an alignment object\n",
    "    '''\n",
    "    expr_list = []\n",
    "               \n",
    "    for expr_file in glob.glob(expr_path + '/*.expr'):\n",
    "        if type == 'std' and 'std' in expr_file:\n",
    "            expr = load_expr(expr_file)\n",
    "            expr_list.append(expr)\n",
    "        elif type == 'exp' and 'std' not in expr_file:\n",
    "            expr = load_expr(expr_file)\n",
    "            expr_list.append(expr)\n",
    "        \n",
    "    # self alignment, generating a list of alignment object\n",
    "    # each experiment has its own self alignment\n",
    "    F1 = exprl2alignment(expr_list)\n",
    "    \n",
    "    # all pairwise alignment of F1\n",
    "    T1 = PairwiseAlignment(F1, Dw, Gw)\n",
    "    \n",
    "    # use pairwise alignment (T1) as the guide to build the overall alignment\n",
    "    A1 = align_with_tree(T1)\n",
    "    \n",
    "    if not os.path.exists('./align'):\n",
    "        os.makedirs('./align') \n",
    "    \n",
    "    with open('./align/' + type + '_align.aln', 'wb') as file_out:\n",
    "        pickle.dump(A1, file_out)\n",
    "    \n",
    "    return A1\n",
    "\n",
    "\n",
    "def std_process(std_align, retention_index = 1900):\n",
    "    '''input:\n",
    "       std_align: alignment object for alkane std\n",
    "       retention_index: int, first alkane index in the standard\n",
    "       \n",
    "       output: DataFrame with retention index and retention time for each alkane\n",
    "    '''\n",
    "    std_peak = std_align.aligned_peaks()\n",
    "    retention_index = 1900\n",
    "    I = []\n",
    "    std_rt = []\n",
    "    for peak in std_peak:\n",
    "        if peak.top_ions(1) != [178]: # sometimes I forgot to use pure EtOAc to prep std, remove \n",
    "            std_rt.append(round(peak.rt/60,3))\n",
    "            I.append(retention_index)\n",
    "            retention_index += 100\n",
    "\n",
    "    return pd.DataFrame({'rt': std_rt, 'I': I})\n",
    "\n",
    "def calc_I(rt, std):\n",
    "    '''calculate retention index\n",
    "    \n",
    "       input:\n",
    "       rt: int, retention time of the peak\n",
    "       std: DataFrame containing std information (rt, I)\n",
    "    '''\n",
    "    \n",
    "    for ind, val in std.iterrows():\n",
    "        if rt >= val['rt']:\n",
    "            index = int(100*(rt - val['rt']) / (std.loc[ind + 1]['rt'] - val['rt']) + val['I'])\n",
    "            return index\n",
    "        \n",
    "def weighted_cos(core_ms, peak_ms, signal_w = 0.51, mz_w = 1.1):\n",
    "    '''\n",
    "    compute weighted cosine similarity between core_ms and peak_ms\n",
    "    \n",
    "    core_ms: mass spectrum object of the LRD core\n",
    "    peak_ms: mass spectrum object of the peak of interest\n",
    "    signal_w: float, the weight used for ion intensities\n",
    "    mz_w: float, the weight used for m/z\n",
    "    '''\n",
    "\n",
    "    ms_start = core_ms.mass_list[0]\n",
    "    ms_end = core_ms.mass_list[-1]\n",
    "    weights = [(i + ms_start) ** mz_w for i in range(ms_end - ms_start + 1)]\n",
    "    \n",
    "    core_weighted = [i**signal_w for i in core_ms.intensity_list]\n",
    "    peak_weighted = [i**signal_w for i in peak_ms.intensity_list]    \n",
    "    \n",
    "    return round(1 - spatial.distance.cosine(core_weighted, peak_weighted, weights),3)\n",
    "\n",
    "def peak_analysis(align, std_align, core,\n",
    "                  retention_index = 1900,\n",
    "                  signal_w = 0.51, mz_w = 1.1,\n",
    "                  ion_threshold = 50000, similar_threshold = 0.5, occur_threshold = 15):\n",
    "    '''analyze peaks identified across experiment\n",
    "    \n",
    "       align, std_align: alignment objects for experiments and stds\n",
    "       core: str, name of the core\n",
    "       retention_index: int, retention index for the first alkane in the standard\n",
    "       signal_w, mz/w: float, weights used to compute similarity\n",
    "       ion_threshold: float, the minimum ion counts to be considered as products\n",
    "       similar_threshold: float, the minimum similarity to be consideread as products\n",
    "       occur_threshold: int, the minimum occurence to be considered as common metabolites\n",
    "       \n",
    "       ouput csv files of rt, area, area ratio with analyzed peak info\n",
    "       return DataFrame with peak objects and analyzed peak info\n",
    "    '''\n",
    "    if not os.path.exists('anlys'):\n",
    "        os.makedirs('anlys')\n",
    "    \n",
    "    std = std_process(std_align, retention_index)\n",
    "    std.to_csv('anlys/std.csv', index = False)   \n",
    "    ###### Retrieve data from alignment object ################################################\n",
    "    # DataFrame of retention time for each peak (rows) in different experiments (columns)\n",
    "    df_rt = round(align.get_peak_alignment(require_all_expr = False),3)\n",
    "    # DataFrame of peak area for each peak (rows) in different experiments (columns)\n",
    "    df_area = round(align.get_area_alignment(require_all_expr = False))\n",
    "    # DataFrame of peak object for each peak (rows) in different experiments (columns)\n",
    "    df_peak = align.get_peaks_alignment(require_all_expr = False)\n",
    "    \n",
    "    # a serie of peak objects with average info (rt, spectrum, etc...) across experiments\n",
    "    ser_peaks_ave = align.aligned_peaks()\n",
    "    ###########################################################################################\n",
    "    \n",
    "    \n",
    "    # Series: frequency of each peak across exp\n",
    "    peak_count = df_rt.count(axis = 1)\n",
    "    # assign names for each peak, padded with up to 3 zeros\n",
    "    peak_ids = [f'{core}_{str(i + 1).zfill(3)}' for i in range(len(peak_count))]\n",
    "    \n",
    "    ###### Core Identification ################################################################\n",
    "    # identify the index of the core from the exp without CYP\n",
    "    # assuming the largest peak (greatest area) is the core\n",
    "    core_index = df_area[core].idxmax()\n",
    "    \n",
    "    # mass spectrum of the core from the exp without CYP\n",
    "    core_ms = normalize_mass_spec(df_peak.iloc[core_index][core].mass_spectrum)\n",
    "    # print core information for manual confirmation\n",
    "    print(f'{core}, retention time: {round(ser_peaks_ave[core_index].rt/60,3)}, id: {peak_ids[core_index]}')\n",
    "    \n",
    "    # a list of bool indicating whether a peak (row) is core LRD\n",
    "    iscore = [False] * len(peak_count)\n",
    "    iscore[core_index] = True\n",
    "\n",
    "    \n",
    "    ###########################################################################################\n",
    "    \n",
    "    ###### Area and Representative Spectrum ###################################################\n",
    "    # DataFrame of peak area ratio for each peak (rows) in different experiments (columns)\n",
    "    # peak area ratio = peak area/core peak area\n",
    "    df_area_ratio = round(df_area/df_area.iloc[core_index], 4)\n",
    "    \n",
    "    # A series of exp_id that has the largest area for each peak\n",
    "    ser_peak_maxid = df_area.idxmax(axis = 1)\n",
    "    # A series of max area and max area ratio for each peak\n",
    "    ser_peak_maxarea = df_area.max(axis = 1)\n",
    "    ser_peak_maxratio = df_area_ratio.max(axis = 1)\n",
    "\n",
    "    \n",
    "    # Use the mass spectrum from the experiment with the greatest area as a representative\n",
    "    msmax = []\n",
    "    for i in range(len(peak_count)):\n",
    "        ms = df_peak.iloc[i][ser_peak_maxid[i]].mass_spectrum\n",
    "        msmax.append(normalize_mass_spec(ms))\n",
    "    \n",
    "    # A DataFrame of mass spectrum (msmax) for each peak (row)\n",
    "    df_msmax = pd.DataFrame({'msmax': msmax})\n",
    "\n",
    "    ############################################################################################\n",
    "    \n",
    "    ###### Average Peak Info ###################################################################\n",
    "    # averaged peak list for each peak: rt, uid (unique ID), msavg, I\n",
    "    rtavgs,uids,msavgs,I = [], [], [], []\n",
    "    for peak in ser_peaks_ave:\n",
    "        peak_rtavg = round(peak.rt/60,3)\n",
    "        rtavgs.append(peak_rtavg)\n",
    "        uids.append(peak.UID)\n",
    "        msavgs.append(peak.mass_spectrum)\n",
    "        I.append(calc_I(peak_rtavg, std))\n",
    "    \n",
    "    # A DataFrame of average mass spectrum (msmax) for each peak (row)\n",
    "    df_msavg = pd.DataFrame({'msavg': msavgs})\n",
    "    ###########################################################################################\n",
    "    \n",
    "    ###### Count occurence of a peak across experiments #######################################\n",
    "    exp_has_peak = []\n",
    "    for i in range(len(peak_count)):\n",
    "        # iloc[index] returns a Series, which has no columns attribute\n",
    "        # iloc[[index]] returns a DataFrame and the following code can work\n",
    "        # notna() returns True or False (for NaN) for each cell\n",
    "        # dot() operation with column names -> only True (not null)'s column names remain\n",
    "        # return a Series object, the index remains the same (does not reset to 0)\n",
    "        exp = df_rt.iloc[[i]].notna().dot(df_rt.iloc[[i]].columns + ',').str.rstrip(',')\n",
    "        # take the value of the Series: exp[i] -> index does not reset to 0\n",
    "        # remove LRD01_ prefix\n",
    "        exp_has_peak.append(exp[i].replace(core + '_', ''))\n",
    "\n",
    "    ###########################################################################################\n",
    "    \n",
    "\n",
    "    ###### Cosine Similarity ################################################################\n",
    "    # a list of similiarity for each peak against the core\n",
    "    similarity = []    \n",
    "    for i in range(len(msmax)):\n",
    "        ms = msmax[i]\n",
    "        similarity.append(weighted_cos(core_ms, ms, signal_w = signal_w, mz_w = mz_w))\n",
    "\n",
    "    \n",
    "    # a list of confidence for each peak (the max observed is used) to be\n",
    "    ## core-side: side products likely come with core\n",
    "    ## common: common metabolites\n",
    "    ## high: very likely to be modified LRD\n",
    "    ## low: unlikely to be modified LRD\n",
    "    ## very-low: singals to be considered\n",
    "    ## core: core\n",
    "    confidence = []\n",
    "    for i in range(len(peak_count)):\n",
    "        if peak_count[i] > occur_threshold:\n",
    "            if similarity[i] > similar_threshold:\n",
    "                confidence.append('core-side')\n",
    "            else:\n",
    "                confidence.append('common')\n",
    "        elif similarity[i] > similar_threshold and ser_peak_maxarea[i] > ion_threshold:\n",
    "            confidence.append('high')\n",
    "        elif similarity[i] > similar_threshold/2 and ser_peak_maxarea[i] > ion_threshold:\n",
    "            confidence.append('low')\n",
    "        else:\n",
    "            confidence.append('low signal')\n",
    "    \n",
    "    # replace the row of core with 'core' in the confidence column\n",
    "    confidence[core_index] = 'core'\n",
    "    \n",
    "    ##########################################################################################\n",
    "\n",
    "    ###### Concatenate all analysis ##########################################################\n",
    "    anlys = pd.DataFrame({'id': peak_ids,\n",
    "                         'UID': uids,\n",
    "                         'RTavg': rtavgs,\n",
    "                         'I': I,\n",
    "                         'core': iscore,\n",
    "                         'product':[False]*len(peak_count),\n",
    "                         'similarity': similarity,\n",
    "                         'max_area': ser_peak_maxarea,\n",
    "                         'max_area_ratio': ser_peak_maxratio,\n",
    "                         'confidence': confidence,\n",
    "                         'counts': peak_count,\n",
    "                         'exp_rep': ser_peak_maxid,\n",
    "                         'exp_list': exp_has_peak,                      \n",
    "                         })\n",
    "    \n",
    "    # if confidence is 'high', label the peak as 'True' in the 'product' column\n",
    "    anlys['product'] = anlys['confidence'] == 'high'\n",
    "    \n",
    "    ### output anlys DataFrame \n",
    "    anlys.to_csv('anlys/anlys.csv', index = False)\n",
    "    \n",
    "    ### output anlys DataFrame with full rt, area, or area ratio to csv files\n",
    "    pd.concat([anlys[['id']], df_rt], axis = 1).to_csv('anlys/rt.csv', index = False)\n",
    "    pd.concat([anlys[['id']], df_area], axis = 1).to_csv('anlys/area.csv', index = False)\n",
    "    pd.concat([anlys[['id']], df_area_ratio], axis = 1).to_csv('anlys/ratio.csv', index = False)\n",
    "    \n",
    "    ### save temp df_peak\n",
    "    df_peak = pd.concat([anlys, df_msmax, df_msavg, df_peak], axis = 1)\n",
    "    df_peak.to_pickle('anlys/peak.df')\n",
    "    \n",
    "    # nist_temp.df only has 'id' columns and other empty attributes as below:\n",
    "    num_peaks = len(peak_ids)\n",
    "    nist = pd.DataFrame({'nist_match': [{}]*num_peaks,\n",
    "                        'match': ['']*num_peaks,\n",
    "                        'formula': ['']*num_peaks,\n",
    "                        'smiles': ['']*num_peaks,\n",
    "                        'cid': ['']*num_peaks,\n",
    "                        'prob': ['']*num_peaks,\n",
    "                        'f_score': ['']*num_peaks,\n",
    "                        'r_score': ['']*num_peaks}\n",
    "                       )\n",
    "    pd.concat([df_peak.iloc[:,0], nist, df_msmax], axis = 1).to_pickle('anlys/nist_temp.df')\n",
    "    #########################################################################################\n",
    "    \n",
    "    print('done with analysis')\n",
    "    return df_peak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3. Functions for NIST search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nist_search(ms, search):\n",
    "    '''\n",
    "    ms: ms, mass spectrum object\n",
    "    search: Engine object to set up nist search\n",
    "    \n",
    "    ouput: a list of dictionary, top 3 hit\n",
    "           dict: name, cid, smiles, formula, prob, f_score, r_score, compound object\n",
    "    '''\n",
    "    names = []\n",
    "    cids = []\n",
    "    smiles = []\n",
    "    formulas = []\n",
    "    probs = []\n",
    "    f_scores = []\n",
    "    r_scores = []\n",
    "    compounds = []\n",
    "    mss = []\n",
    "    \n",
    "    nist_hit = search.full_search_with_ref_data(ms)\n",
    "    num_hit = len(nist_hit)\n",
    "    \n",
    "    ## each item in nist_hit is a tuple: (search_result, reference_data)\n",
    "    ## search_result is related to the scoring\n",
    "    ## reference_data is related to the molecular information\n",
    "    \n",
    "    # only keep at most three hits\n",
    "    for i in range(min(3, num_hit)):\n",
    "        hit = nist_hit[i]\n",
    "\n",
    "        ## use the default name to search cid from pubchem first\n",
    "        ## return a list of possible Compound objects\n",
    "        ## sleep 0.5 seconds to avoid TimeoutError\n",
    "        pubchem_hit = pcp.get_compounds(hit[0].name,'name', listkey_count=1)\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        ## if first search fails:\n",
    "        ## search on pubchem with at most five other synonyms\n",
    "        if pubchem_hit == []:\n",
    "            for j in range(min(5, len(hit[1].synonyms))):\n",
    "                if pubchem_hit == []:\n",
    "                    altname = hit[1].synonyms[j]\n",
    "                    if '$:28' in altname:\n",
    "                        ## inchikey sometimes is a synonum in the nist search result\n",
    "                        ## remove the prefix \"$:28\" before search\n",
    "                        pubchem_hit = pcp.get_compounds(altname.replace('$:28',''),'inchikey', listkey_count=1)\n",
    "                        time.sleep(0.5)\n",
    "                    else:\n",
    "                        pubchem_hit = pcp.get_compounds(altname,'name', listkey_count=1)\n",
    "                        time.sleep(0.5)\n",
    "        \n",
    "        \n",
    "        formulas.append(hit[1].formula)\n",
    "        probs.append(hit[0].hit_prob)\n",
    "        f_scores.append(hit[0].match_factor)\n",
    "        r_scores.append(hit[0].reverse_match_factor)\n",
    "        mss.append(hit[1].mass_spec)\n",
    "        \n",
    "        ## no hit from pubchem, use the name from NIST database, leave other fields empty\n",
    "        if pubchem_hit == []:           \n",
    "            compounds.append(None)\n",
    "            cids.append(None)\n",
    "            smiles.append(None)\n",
    "            names.append(hit[0].name)   \n",
    "            \n",
    "        ## with pubchem hit, use the first compound in the list\n",
    "        else:\n",
    "            compounds.append(pubchem_hit[0])             \n",
    "            cids.append(int(pubchem_hit[0].cid))\n",
    "            smiles.append(pubchem_hit[0].isomeric_smiles)\n",
    "            if pubchem_hit[0].synonyms != []:\n",
    "                names.append(pubchem_hit[0].synonyms[0])\n",
    "            else:\n",
    "                names.append(pubchem_hit[0].iupac_name)\n",
    "            \n",
    "    ## repeated search can crash the code\n",
    "    \n",
    "    \n",
    "    return {'name': names,\n",
    "            'cid': cids,\n",
    "            'smiles': smiles,\n",
    "            'formula': formulas,\n",
    "            'prob': probs,\n",
    "            'f_score': f_scores,\n",
    "            'r_score': r_scores,\n",
    "            'ms': mss\n",
    "            }\n",
    "    \n",
    "\n",
    "def update_nist(path, search):\n",
    "    '''\n",
    "    Do the NIST search for each peak in the dataframe, using the mass spectrum object\n",
    "    Save the temp df every 5 peaks (sometimes the code would crash)\n",
    "    Can restart with the temp df\n",
    "    \n",
    "    input:\n",
    "    path: str, where the file nist_temp.df locates\n",
    "    search: Engine object to set up nist search\n",
    "    \n",
    "    output:\n",
    "    df: dataframe (nist_temp.df) with updated search results\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df = pd.read_pickle(path)\n",
    "    \n",
    "    peak_ids = list(df['id'])\n",
    "    count = 0\n",
    "    \n",
    "    for i, val in df.iterrows():\n",
    "        \n",
    "        ## peak not searched yet\n",
    "        if val['match'] == '':\n",
    "            \n",
    "            # nist search using msmax, not msavg\n",
    "            ms = val['msmax']            \n",
    "            nist_match = nist_search(ms, search)\n",
    "            \n",
    "            # record the top match to the spreadsheet\n",
    "            df.at[i,'match'] = nist_match['name'][0]\n",
    "            df.at[i,'formula'] = nist_match['formula'][0]\n",
    "            df.at[i,'smiles'] = nist_match['smiles'][0]\n",
    "            df.at[i,'cid'] = nist_match['cid'][0]\n",
    "            df.at[i,'prob'] = nist_match['prob'][0]\n",
    "            df.at[i,'f_score'] = nist_match['f_score'][0]\n",
    "            df.at[i,'r_score'] = nist_match['r_score'][0]\n",
    "            # keep all the search result (a list of dictionaries)\n",
    "            df.at[i,'nist_match'] = nist_match\n",
    "            \n",
    "            top_formula = nist_match['formula'][0]\n",
    "            top_match = nist_match['name'][0]\n",
    "            top_f = nist_match['f_score'][0]\n",
    "            top_r = nist_match['r_score'][0]\n",
    "            cid = nist_match['cid'][0]\n",
    "            \n",
    "            print(f'top hit of {peak_ids[i]}: {cid}, {top_formula}, {top_match}; score: {top_f}/{top_r}')\n",
    "            count += 1\n",
    "            \n",
    "        ## peak has been searched\n",
    "        else:\n",
    "            print(f'{peak_ids[i]} has been analyzed')\n",
    "        \n",
    "        # process and pickle every 5 entries\n",
    "        if count == 5:\n",
    "            df.to_pickle('./anlys/nist_temp.df')\n",
    "            count = 0\n",
    "            \n",
    "    df.to_pickle('./anlys/nist_temp.df')\n",
    "    print('all peaks analyzed and saved')\n",
    "           \n",
    "    anlys = pd.read_csv('./anlys/anlys.csv')\n",
    "    for key in ['match', 'formula', 'smiles', 'cid', 'prob', 'f_score', 'r_score']:\n",
    "        anlys[key] = df[key]\n",
    "        \n",
    "    anlys.to_csv('./anlys/anlys_nist.csv', index = False)\n",
    "    anlys.to_csv('anlys_rev.csv', index = False)\n",
    "    \n",
    "    # this file contains all the objects and full nist_match\n",
    "    df_peak = pd.read_pickle('anlys/peak.df')\n",
    "    for key in ['match', 'formula', 'smiles', 'cid', 'prob', 'f_score', 'r_score', 'nist_match']:\n",
    "        df_peak[key] = df[key]\n",
    "        df_peak.to_pickle('anlys/peak_nist.df')\n",
    "    print('peak data updated (pickled)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 4. Functions for generating a pdf report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def MS_compare(width, length, style, core_id, peak_id, peak_ms, compare_ms):\n",
    "    '''\n",
    "    width: float\n",
    "    length: float\n",
    "    style: int, 0: peak vs core, 1-3: NIST match\n",
    "    core_id: str \n",
    "    peak_id: str\n",
    "    peak_ms: DataFrame for the peak (mass, counts)\n",
    "    compare_ms: DataFrame for the core or NIST match (mass, counts)\n",
    "    \n",
    "    '''\n",
    "    plot_color = ['black','red','blue','blue','blue']\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1, figsize = (width, length), constrained_layout=False)\n",
    "    \n",
    "    if style == 0:    \n",
    "        ax.set_title(f'{peak_id} vs {core_id}', loc = 'left')\n",
    "    else:\n",
    "        ax.set_title(f'{peak_id}    NIST match {style}', loc = 'left')\n",
    "    \n",
    "    for i, raw_data in enumerate([peak_ms, compare_ms]):\n",
    "        raw_data.counts = raw_data.counts/max(raw_data.counts)*100\n",
    "        raw_data.loc[raw_data.counts < max(raw_data.counts)* 0.01, 'counts'] = 0\n",
    "        \n",
    "        ax.bar(raw_data.mass,\n",
    "               raw_data.counts * (-2*i + 1),    # (-2*i + 1) flips the compare_ms\n",
    "               color = plot_color[i*style + i], # i*style + i to select colors \n",
    "               linewidth = 0,\n",
    "               width = 1\n",
    "               )\n",
    "        \n",
    "        # only conisder ions > 10 for labeling\n",
    "        top_ions = raw_data.loc[raw_data.counts > 10].sort_values('counts', ascending = False)\n",
    "        \n",
    "        # use prevent label overlapping with borders\n",
    "        blocks = [(355, 360),\n",
    "                  (0, 45)]\n",
    "        # from the highest ions,\n",
    "        # if the ion resides outside the blocks, label the ions\n",
    "        # 'block' a range of mass that has no space for text labels\n",
    "        # repeat until go through all the top_ions\n",
    "\n",
    "        counts = 0\n",
    "        add = True\n",
    "        for ind, val in top_ions.iterrows():\n",
    "            if counts <= 5:         # label at most 5 ions\n",
    "                if any(lower <= val.mass - 10 <= upper for (lower, upper) in blocks):\n",
    "                    add = False\n",
    "                elif any(lower <= val.mass + 10 <= upper for (lower, upper) in blocks):\n",
    "                    add = False                    \n",
    "                else:\n",
    "                    ax.text(val.mass,\n",
    "                            (val.counts + 0.04)*(-2*i + 1) - 35*i,\n",
    "                            # 0.04 to avoid ion label overlaps with the signals\n",
    "                            # (-2*i + 1) to flip compare_ms\n",
    "                            # - 35*i to ship ion labels in compare_ms down\n",
    "                            str(int(val.mass)),\n",
    "                            ha='center',\n",
    "                            va='bottom',\n",
    "                            color = plot_color[i*style + i]\n",
    "                            )\n",
    "                    counts += 1\n",
    "\n",
    "                blocks.append((int(val.mass - 10),\n",
    "                                int(val.mass + 10)))\n",
    "        \n",
    "    ax.axhline(color = 'black')\n",
    "    ax.set_ylim(-140,140)\n",
    "    ax.set_xlim(40,360)\n",
    "    ax.set_xticks([50,100,150,200,250,300,350])\n",
    "    ax.set_yticks([100,50,0,-50,-100])\n",
    "    ax.set_yticklabels([str(abs(x)) for x in ax.get_yticks()])\n",
    "    \n",
    "    ax.set_xlabel('$m/z$')\n",
    "    ax.set_ylabel('Ion counts (%)')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    imgdata = BytesIO()\n",
    "    fig.savefig(imgdata, format='pdf')\n",
    "    imgdata.seek(0)\n",
    "    plt.close()\n",
    "    return imgdata\n",
    "\n",
    "def GC_trace(width, length, tic, exp, RTavg, zoom = False):\n",
    "    '''\n",
    "    width: float\n",
    "    length: float\n",
    "    tic: DataFrame (time, counts)\n",
    "    exp: str, the experiment to generate this TIC\n",
    "    RTavg: float, retention time of the peak\n",
    "    zoom: Bool, zoom in around the peak or not\n",
    "\n",
    "    '''\n",
    "    fig, ax = plt.subplots(1,1, figsize = (width, length), constrained_layout=False)\n",
    "    # differece between RTavg (avg from multiple exps) and recored RT\n",
    "    tic['dif'] = abs(tic.time - RTavg)\n",
    "    \n",
    "    x_min, x_max = 5, 14\n",
    "    if zoom:\n",
    "        x_min, x_max = max(5, RTavg - 1), min(14, RTavg + 1)\n",
    "        exp = 'zoom in'       \n",
    "        \n",
    "    tic_trim = tic.loc[(tic.time >= x_min) & (tic.time <= x_max)]\n",
    "    \n",
    "    # the max ion (product) from the closest 6 RT to RTavg\n",
    "    pdt_ion = tic_trim.sort_values(by = 'dif')[:6].counts.max()\n",
    "    \n",
    "    if zoom:  \n",
    "        y_max = pdt_ion * 1.5        \n",
    "    else:\n",
    "        y_max = tic_trim.counts.max() * 1.2\n",
    "        \n",
    "\n",
    "    \n",
    "    ax.plot(tic_trim.time,\n",
    "            tic_trim.counts,\n",
    "            color = 'black')\n",
    "    \n",
    "    # mark the peak with an arrow\n",
    "    ax.annotate(text = '',\n",
    "                xy = (RTavg, pdt_ion),\n",
    "                xytext=(RTavg, pdt_ion + 0.25*y_max),\n",
    "                arrowprops=dict(arrowstyle='-|>', color = 'red')\n",
    "               )\n",
    "\n",
    "    ax.set_xlim(x_min, x_max)    \n",
    "    plotscale = 10**math.floor((math.log10(y_max)))\n",
    "    ax.set_ylim(-y_max*0.05, y_max)\n",
    "    \n",
    "    ax.yaxis.set_major_locator(MaxNLocator(3))\n",
    "    ax.set_yticks(ax.get_yticks())\n",
    "    ax.set_yticklabels(['{:,.1f}'.format(x) for x in ax.get_yticks()/plotscale])\n",
    "    ax.set_ylim(-y_max*0.05, y_max)    \n",
    "    \n",
    "    a = math.floor((math.log10(y_max)))\n",
    "    ax.set_ylabel(f'Ion counts (x $10^{a}$)')\n",
    "    ax.set_xlabel('Retention time (min)')\n",
    "    ax.set_title(exp, loc = 'left')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    imgdata = BytesIO()\n",
    "    fig.savefig(imgdata, format='pdf')\n",
    "    imgdata.seek(0)\n",
    "    plt.close()\n",
    "    return imgdata\n",
    "\n",
    "\n",
    "def CYP_heatmap(width, height, core_id, df_ratio, peak_id):\n",
    "    '''\n",
    "    Generate a heat map showing the ratio of peak/core for every experiment\n",
    "    \n",
    "    input:\n",
    "    width: float\n",
    "    height: float\n",
    "    core_id: str, LRD core\n",
    "    df_ratio: DataFrame of ratio\n",
    "    peak_id: str, id of the peak\n",
    "    \n",
    "    input is a DataFrame, row name is ratio, and columns are experiment name\n",
    "    '''\n",
    "    # add nan to non-existene experiment\n",
    "    \n",
    "    exp_num = len(df_ratio.columns)   ### including LRD core + no CYP\n",
    "         \n",
    "    df_ratio = round(df_ratio.loc[peak_id]*100)\n",
    "    \n",
    "    # fill the array to 10x items\n",
    "    # this is for plotting the heatmap\n",
    "    a = np.append(df_ratio.to_numpy(), [np.nan]*(exp_num % 10))\n",
    "    heatmap = pd.DataFrame(a.reshape(len(a)//10,10))\n",
    "    \n",
    "    # fill the array to 10x items, only the padded cells have values\n",
    "    # this is for coloring the padded cells black\n",
    "    a_str = np.append(df_ratio.astype(str).to_numpy(), [np.nan]*(exp_num % 10))\n",
    "    heatmap_str = pd.DataFrame(a_str.reshape(len(a)//10,10))\n",
    "    b = pd.DataFrame([[1]*10]*(len(a)//10))\n",
    "    no_cyp = b.mask(heatmap_str.notna())\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(width,height))\n",
    "    \n",
    "    # color the padded cells black\n",
    "    sns.heatmap(no_cyp,\n",
    "                cmap = 'gray',\n",
    "                linewidth = 0,\n",
    "                cbar = False,\n",
    "                annot = False\n",
    "               )\n",
    "    \n",
    "    # plot the heat map\n",
    "    sns.heatmap(heatmap,\n",
    "                cmap = 'YlGnBu',\n",
    "                linecolor = 'black',\n",
    "                linewidth = 0.8,\n",
    "                cbar = False,\n",
    "                vmin = 0,\n",
    "                vmax = 20,\n",
    "                xticklabels = [i for i in range(10)],\n",
    "                yticklabels = [i for i in range(len(heatmap))],\n",
    "                annot = True,\n",
    "                fmt = 'g',\n",
    "                annot_kws = {'fontsize':6, 'fontfamily':'arial', 'va':'center', 'ha':'center'},\n",
    "                )\n",
    "    ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize = 8, fontfamily = 'arial', rotation = 0)\n",
    "    ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize = 8, fontfamily = 'arial', rotation = 0)\n",
    "    ax.set_title(f'{peak_id}/{core_id} by CYP (%)', fontsize = 8, fontfamily = 'arial')\n",
    "    ax.set_xlabel('CYPs (00 = no CYP)')\n",
    "     \n",
    "\n",
    "    for _, spine in ax.spines.items():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(0.8)\n",
    "        spine.set_color('black')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    imgdata = BytesIO()\n",
    "    plt.savefig(imgdata, format='pdf')\n",
    "    imgdata.seek(0)\n",
    "    plt.close()\n",
    "    return imgdata\n",
    "          \n",
    "            \n",
    "ps = ParagraphStyle('title_paragraph', fontName = 'Helvetica-Bold', fontSize = 12)\n",
    "ps_red = ParagraphStyle('title_paragraph', fontName = 'Helvetica-Bold', fontSize = 12,\n",
    "                        textColor= 'red')\n",
    "\n",
    "ts_product = TableStyle([\n",
    "                      ('LINEABOVE',(0,-1), (-1,-1), 0.5, colors.black),\n",
    "                      ('LINEABOVE',(0,0), (-1,0), 0.75, colors.black),\n",
    "                      ('LINEBELOW',(0,-1), (-1,-1), 0.75, colors.black),\n",
    "                      ('SIZE', (0,0), (-1,-1), 8),\n",
    "                      ('TOPPADDING', (0,0), (-1,-1), 1),\n",
    "                      ('BOTTOMPADDING', (0,0), (-1,-1), 1),\n",
    "                      ('ALIGN',(0,0),(-1,-1), 'CENTER'),\n",
    "                      ])\n",
    "ts_info = TableStyle([\n",
    "                      ('LINEBEFORE',(1,0),(1,-1), 0.8, colors.black),\n",
    "                      ('SIZE', (0,0), (-1,-1), 8),\n",
    "                      ('TOPPADDING', (0,0), (-1,-1), 1.5),\n",
    "                      ('BOTTOMPADDING', (0,0), (-1,-1), 1.5),\n",
    "                      ('ALIGN',(0,0),(0,-1), 'RIGHT'),\n",
    "                      ])\n",
    "ts_info_color = TableStyle([\n",
    "                      ('LINEBEFORE',(1,0),(1,-1), 0.5, colors.black),\n",
    "                      ('SIZE', (0,0), (-1,-1), 8),\n",
    "                      ('TOPPADDING', (0,0), (-1,-1), 1),\n",
    "                      ('BOTTOMPADDING', (0,0), (-1,-1), 1),\n",
    "                      ('ALIGN',(0,0),(0,-1), 'RIGHT'),\n",
    "                       # color the score red if the score is above 800\n",
    "                      ('TEXTCOLOR', (-1,-1), (-1,-1), colors.red)\n",
    "                      ])\n",
    "\n",
    "def form_xo_reader(imgdata):\n",
    "    page, = PdfReader(imgdata).pages\n",
    "    return pagexobj(page)\n",
    "\n",
    "\n",
    "class PdfImage(Flowable):\n",
    "    def __init__(self, img_data, width, height):\n",
    "        self.img_width = width\n",
    "        self.img_height = height\n",
    "        self.img_data = img_data\n",
    "\n",
    "    def wrap(self, width, height):\n",
    "        return self.img_width, self.img_height\n",
    "\n",
    "    def drawOn(self, canv, x, y, _sW=0):\n",
    "        if _sW > 0 and hasattr(self, 'hAlign'):\n",
    "            a = self.hAlign\n",
    "            if a in ('CENTER', 'CENTRE', TA_CENTER):\n",
    "                x += 0.5*_sW\n",
    "            elif a in ('RIGHT', TA_RIGHT):\n",
    "                x += _sW\n",
    "            elif a not in ('LEFT', TA_LEFT):\n",
    "                raise ValueError(\"Bad hAlign value \" + str(a))\n",
    "        canv.saveState()\n",
    "        img = self.img_data\n",
    "        if isinstance(img, PdfDict):\n",
    "            xscale = self.img_width / img.BBox[2]\n",
    "            yscale = self.img_height / img.BBox[3]\n",
    "            canv.translate(x, y)\n",
    "            canv.scale(xscale, yscale)\n",
    "            canv.doForm(makerl(canv, img))\n",
    "        else:\n",
    "            canv.drawImage(img, x, y, self.img_width, self.img_height)\n",
    "        canv.restoreState()\n",
    "        \n",
    "def initate_report(core, out_path):\n",
    "    '''\n",
    "    set the frames (the size for each block in a page) for the report \n",
    "    \n",
    "    input:\n",
    "    core: str, LRD core\n",
    "    out_path: path for the output pdf\n",
    "    \n",
    "    output:\n",
    "    doc: BaseDocTemplate object\n",
    "    '''\n",
    "    \n",
    "    # set up the template for a page\n",
    "    # a title frame\n",
    "    # a 1:2 top frame, left for MS comparison with the core, right for RT, I, similariy, and experiments with this peak\n",
    "    # a 1:1:1 bottom frame to display 3 nist search results\n",
    "    # left: MS comparison with the database\n",
    "    # middle: structure\n",
    "    # right: name, cid, formula, mw, probability, and scores\n",
    "    # inch is the unit defined in the package\n",
    "    doc = BaseDocTemplate(f'{out_path}{core}_report.pdf',\n",
    "                      pagesize=letter,\n",
    "                      rightMargin=0.5*inch,\n",
    "                      leftMargin=0.5*inch,\n",
    "                      topMargin=0.5*inch,\n",
    "                      bottomMargin=0.5*inch,\n",
    "                      showBoundary = 0\n",
    "                      )\n",
    "\n",
    "    frameWidth = doc.width / 3\n",
    "    frameHeight = doc.height * 0.96 / 5\n",
    "\n",
    "    # title\n",
    "    titleframe = Frame(x1=doc.leftMargin,\n",
    "                       y1=doc.bottomMargin + doc.height * 0.96,\n",
    "                       width= doc.width,\n",
    "                       height= doc.height * 0.04\n",
    "                      )\n",
    "    # ms comparison with core\n",
    "    topleftframe = Frame(x1=doc.leftMargin,\n",
    "                         y1=doc.bottomMargin + frameHeight*4,\n",
    "                         width=frameWidth*1.1,\n",
    "                         height=frameHeight)\n",
    "    # peak info\n",
    "    topmidframe = Frame(x1=doc.leftMargin + frameWidth*1.1,\n",
    "                        y1=doc.bottomMargin + frameHeight*4,\n",
    "                        width=frameWidth*0.8,\n",
    "                        height=frameHeight)\n",
    "    \n",
    "    # empty\n",
    "    toprightframe = Frame(x1=doc.leftMargin + frameWidth*1.9,\n",
    "                          y1=doc.bottomMargin + frameHeight*4,\n",
    "                          width=frameWidth*1.1,\n",
    "                          height=frameHeight)\n",
    "    \n",
    "    # gc trace\n",
    "    midleftframe = Frame(x1=doc.leftMargin,\n",
    "                         y1=doc.bottomMargin + frameHeight*3,\n",
    "                         width=frameWidth*1.1,\n",
    "                         height=frameHeight)\n",
    "    \n",
    "    # gc trace zoomin\n",
    "    midmidframe = Frame(x1=doc.leftMargin + frameWidth*1.1,\n",
    "                         y1=doc.bottomMargin + frameHeight*3,\n",
    "                         width=frameWidth*0.8,\n",
    "                         height=frameHeight)\n",
    "    \n",
    "    # CYP heatmap\n",
    "    midrightframe = Frame(x1=doc.leftMargin + frameWidth * 1.9,\n",
    "                          y1=doc.bottomMargin + frameHeight*3,\n",
    "                          width=frameWidth*1.1,\n",
    "                          height=frameHeight)\n",
    "    \n",
    "    # ms comparison with NIST match\n",
    "    bottomleftframe = Frame(x1=doc.leftMargin,\n",
    "                            y1=doc.bottomMargin,\n",
    "                            width=frameWidth*1.1,\n",
    "                            height=frameHeight * 3)\n",
    "    # chemical structure of NIST match\n",
    "    bottommidframe = Frame(x1=doc.leftMargin + frameWidth * 1.1,\n",
    "                           y1=doc.bottomMargin,\n",
    "                           width=frameWidth*0.8,\n",
    "                           height=frameHeight * 3)\n",
    "    # info of NIST match\n",
    "    bottomrightframe = Frame(x1=doc.leftMargin + frameWidth * 1.9,\n",
    "                             y1=doc.bottomMargin,\n",
    "                             width=frameWidth*1.1,\n",
    "                             height=frameHeight * 3)\n",
    "\n",
    "    frame_list = [titleframe,\n",
    "                  topleftframe,\n",
    "                  topmidframe,\n",
    "                  toprightframe,\n",
    "                  midleftframe,\n",
    "                  midmidframe,\n",
    "                  midrightframe,\n",
    "                  bottomleftframe,\n",
    "                  bottommidframe,\n",
    "                  bottomrightframe\n",
    "                 ]\n",
    "    \n",
    "    doc.addPageTemplates([PageTemplate(id='frames',\n",
    "                                       frames=frame_list)])\n",
    "\n",
    "    return(doc)\n",
    "\n",
    "def generate_report(peak, peak_id, core_id, mscore, int_path, tic_path):\n",
    "    '''\n",
    "    \n",
    "    input:\n",
    "    peak: dataframe, all information of a peak\n",
    "    peak_id: str, id of the peak\n",
    "    core_id: str, LRD core\n",
    "    mscore: MSHtoL object, mass spectrum for the core (flipped)\n",
    "    int_path: str, the folder with all intermediate files\n",
    "    tic_path: str, the folder with all tic files\n",
    "    '''\n",
    "    \n",
    "    print('generating report of ' + peak_id)\n",
    "    \n",
    "    df_ratio = pd.read_csv(f'{int_path}ratio.csv', index_col = 'id')\n",
    "    \n",
    "    ### title ###############################################################\n",
    "    page_title = Paragraph(peak_id, style = ps)\n",
    "    titleframe_story = [page_title,\n",
    "                        FrameBreak()] # jump to the next frame\n",
    "    #########################################################################\n",
    "    \n",
    "   \n",
    "    ### topleft frame: Peak vs Core mass spectrua comparison ################\n",
    "    # peak MS\n",
    "    peak_ms = pd.DataFrame({'mass': peak.msmax.mass_list,\n",
    "                            'counts': peak.msmax.intensity_list})\n",
    "    imgdata = MS_compare(width = 2.5,\n",
    "                         length = 1.75,\n",
    "                         style = 0,\n",
    "                         core_id = core_id,\n",
    "                         peak_id = peak_id,\n",
    "                         peak_ms = peak_ms,\n",
    "                         compare_ms = mscore\n",
    "                         )\n",
    "\n",
    "    image = form_xo_reader(imgdata)   # cannot use Pdfreader directly\n",
    "    topleftframe_story = [PdfImage(image, width = 2.5*inch, height = 1.75*inch),\n",
    "                           FrameBreak()]\n",
    "    #########################################################################\n",
    "    \n",
    "    ### topmid frame: UID, RTavg, I, similarity info ########################\n",
    "    \n",
    "    ### create a table (a list of list)\n",
    "    pdt_table = []\n",
    "    pdt_table.append(['UID', peak.UID])\n",
    "    pdt_table.append(['RTavg', str(peak.RTavg) + ' min'])\n",
    "    try:\n",
    "        pdt_table.append(['I', int(peak.I)])\n",
    "    except ValueError:\n",
    "        pdt_table.append(['I', ''])\n",
    "    pdt_table.append(['similarity', peak.similarity])\n",
    "\n",
    "    \n",
    "    tbl_product = Table(pdt_table, style = ts_info, hAlign = 'LEFT')\n",
    "    \n",
    "    topmidframe_story = [Spacer(1,0.5*inch), tbl_product, FrameBreak()]   \n",
    "    ######################################################################### \n",
    "    \n",
    "    ### topright frame: automatic assignment ################################\n",
    "    \n",
    "    assignment = Paragraph(f'auto assignment: {peak.confidence}', style = ps)\n",
    "\n",
    "    if peak['product']:\n",
    "        product = Paragraph('Product', style = ps_red)\n",
    "    elif peak.core:\n",
    "        product = Paragraph('Core', style = ps_red)\n",
    "    else:\n",
    "        product = Paragraph('', style = ps_red)\n",
    "    \n",
    "    toprightframe_story = [Spacer(1,0.5*inch),\n",
    "                           assignment,\n",
    "                           Spacer(1,0.3*inch),\n",
    "                           product,\n",
    "                           FrameBreak()]\n",
    "    ######################################################################### \n",
    "   \n",
    "    ### midleft, midmid frame: GC-MS traces, full and zoom ##################\n",
    "\n",
    "    if peak.core:\n",
    "        tic_file = pd.read_csv(f'{tic_path}{core_id}.csv')\n",
    "        exp_rep = core_id\n",
    "    else:\n",
    "        tic_file = pd.read_csv(f'{tic_path}{peak.exp_rep}.csv')\n",
    "        exp_rep = peak.exp_rep\n",
    "            \n",
    "    imgdata = GC_trace(width = 2.5,\n",
    "                       length = 1.75,\n",
    "                       tic = tic_file,\n",
    "                       exp = exp_rep,\n",
    "                       RTavg = peak.RTavg,\n",
    "                       zoom = False)\n",
    "    image = form_xo_reader(imgdata)   # cannot use Pdfreader directly\n",
    "    img_gc_trace = PdfImage(image, width = 2.5*inch, height = 1.75*inch)\n",
    "    \n",
    "    midleftframe_story = [img_gc_trace,\n",
    "                         FrameBreak()]\n",
    "\n",
    "    imgdata = GC_trace(width = 1.8,\n",
    "                       length = 1.75,\n",
    "                       tic = tic_file,\n",
    "                       exp = exp_rep,\n",
    "                       RTavg = peak.RTavg,\n",
    "                       zoom = True)\n",
    "    image = form_xo_reader(imgdata)   # cannot use Pdfreader directly\n",
    "    img_gc_trace = PdfImage(image, width = 1.8*inch, height = 1.75*inch)\n",
    "    \n",
    "    midmidframe_story = [img_gc_trace,\n",
    "                         FrameBreak()]    \n",
    "    \n",
    "    #########################################################################\n",
    "        \n",
    "    ### midright frame: heatmap of peak detection ###########################\n",
    "    \n",
    "    imgdata = CYP_heatmap(width = 2.5, height = 1.75,\n",
    "                          core_id = core_id,\n",
    "                          df_ratio = df_ratio,\n",
    "                          peak_id = peak_id)\n",
    "    image = form_xo_reader(imgdata)\n",
    "    img_heatmap = PdfImage(image, width=2.5*inch, height=1.75*inch)\n",
    "    midrightframe_story = [img_heatmap, FrameBreak()]\n",
    "    \n",
    "    #########################################################################\n",
    "    \n",
    "   \n",
    "    ### bottom frames #######################################################\n",
    "    bottomleftframe_story = []\n",
    "    bottommidframe_story = [Spacer(1,0.05*inch)]\n",
    "    bottomrightframe_story = [Spacer(1,0.2*inch)]\n",
    "\n",
    "    num_matches = len(peak.nist_match['name'])\n",
    "    for j in range(num_matches):\n",
    "\n",
    "        ### bottomleftframe #############################################\n",
    "        match = normalize_mass_spec(peak.nist_match['ms'][j])\n",
    "        match_ms = pd.DataFrame({'mass': match.mass_list, 'counts': match.intensity_list})\n",
    "        # with a label matchx on the bottom right corner\n",
    "        imgdata = MS_compare(width = 2.5,\n",
    "                             length = 1.75,\n",
    "                             style = j + 1,\n",
    "                             core_id = core_id,\n",
    "                             peak_id = peak_id,\n",
    "                             peak_ms = peak_ms,\n",
    "                             compare_ms = match_ms\n",
    "                             )\n",
    "        \n",
    "        image = form_xo_reader(imgdata)   # cannot use Pdfreader directly\n",
    "        img_core_match = PdfImage(image, width = 2.5*inch, height = 1.75*inch)\n",
    "        bottomleftframe_story.append(img_core_match)\n",
    "        bottomleftframe_story.append(Spacer(1,0.1*inch))\n",
    "\n",
    "        ### bottommidframe ##############################################\n",
    "        smiles = peak.nist_match['smiles'][j]\n",
    "        try:\n",
    "            m = Chem.MolFromSmiles(smiles)\n",
    "        except TypeError:\n",
    "            m = Chem.MolFromSmiles('')\n",
    "        Draw.MolToFile(m, str(j) + '.svg', size = (360, 300))\n",
    "        drawing = svg2rlg(str(j) + '.svg')\n",
    "        renderPDF.drawToFile(drawing, str(j) + '.pdf')\n",
    "        # this intermediate pdf needs to be removed later\n",
    "        \n",
    "        image = form_xo_reader(str(j) + '.pdf')   # cannot use Pdfreader directly\n",
    "        img_nist_match = PdfImage(image, width = 1.8*inch, height = 1.5*inch)\n",
    "\n",
    "        bottommidframe_story.append(img_nist_match)\n",
    "        if j != 2:\n",
    "            bottommidframe_story.append(Spacer(1,0.35*inch)) \n",
    "\n",
    "        ### bottomrightframe ############################################\n",
    "        info_table = []\n",
    "        info_table.append(['name', peak.nist_match['name'][j]])\n",
    "        info_table.append(['cid', peak.nist_match['cid'][j]])\n",
    "        info_table.append(['formula', peak.nist_match['formula'][j]])\n",
    "        info_table.append(['mw', round(ExactMolWt(m),1)])\n",
    "        info_table.append(['prob', str(peak.nist_match['prob'][j])])\n",
    "        info_table.append(['score', str(peak.nist_match['f_score'][j]) + '/' + str(peak.nist_match['r_score'][j])])\n",
    "\n",
    "        if peak.nist_match['f_score'][j] > 800 and peak.nist_match['r_score'][j] > 800:\n",
    "            tbl_info = Table(info_table, style = ts_info_color, hAlign = 'LEFT')\n",
    "        else:\n",
    "            tbl_info = Table(info_table, style = ts_info, hAlign = 'LEFT')                \n",
    "\n",
    "        bottomrightframe_story.append(tbl_info)\n",
    "        if j != 2:\n",
    "            bottomrightframe_story.append(Spacer(1,0.6*inch))     \n",
    "\n",
    "    bottommidframe_story.append(FrameBreak())\n",
    "    bottomrightframe_story.append(FrameBreak())\n",
    "    bottomleftframe_story.append(FrameBreak())\n",
    "    #########################################################################\n",
    "    \n",
    "    story = titleframe_story + topleftframe_story + topmidframe_story + toprightframe_story\n",
    "    story = story + midleftframe_story + midmidframe_story + midrightframe_story\n",
    "    story = story + bottomleftframe_story + bottommidframe_story + bottomrightframe_story\n",
    "    \n",
    "    print('\\t...done')\n",
    "    \n",
    "    return(story)\n",
    "\n",
    "\n",
    "\n",
    "def build_pdf_report(core_id, int_path, tic_path, out_path):\n",
    "    '''\n",
    "    generate human-readable pdf report for the screening results\n",
    "    \n",
    "    input:\n",
    "    core_id: str, LRD core\n",
    "    int_path: str, path for the peak_nist.df\n",
    "    tic_path: str, path for all tic files\n",
    "    out_path: str, path for the output\n",
    "    \n",
    "    output:\n",
    "    a pdf file for the screening results\n",
    "    '''\n",
    "    \n",
    "    df_peak = pd.read_pickle(f'{int_path}peak_nist.df')\n",
    "\n",
    "    # core MS\n",
    "    core = df_peak.loc[df_peak['core'] == True].iloc[0]['msmax']\n",
    "    core_ms = pd.DataFrame({'mass': core.mass_list, 'counts': core.intensity_list})\n",
    "    \n",
    "    doc = initate_report(core_id, out_path = out_path)\n",
    "    \n",
    "    story = []\n",
    "    \n",
    "    for i, peak in df_peak.iterrows():\n",
    "        if i == 0:\n",
    "            print(f'start generating report: {out_path}{core_id}_report.pdf')\n",
    "            print(f'total {len(df_peak)} peaks')\n",
    "\n",
    "        story = story + generate_report(peak = peak,\n",
    "                                        peak_id = peak.id,\n",
    "                                        core_id = core_id,\n",
    "                                        mscore = core_ms,\n",
    "                                        int_path = int_path,\n",
    "                                        tic_path = tic_path)\n",
    "\n",
    "    doc.build(story)\n",
    "\n",
    "\n",
    "    # remove the intermediary structure pdf file\n",
    "    for j in range(3):\n",
    "        if os.path.exists(str(j) + '.pdf'): os.remove(str(j) + '.pdf')\n",
    "        if os.path.exists(str(j) + '.svg'): os.remove(str(j) + '.svg')\n",
    "        \n",
    "    print('\\nfinishing generating the report')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 5. Functions for Finalizing product selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_heatmap(core_id, heatmap, info, out_path, annot = False):\n",
    "    '''\n",
    "    core_id: str\n",
    "    heatmap: DataFrame\n",
    "    info: DataFrame\n",
    "    outpath: str\n",
    "    \n",
    "    output:\n",
    "    a pdf file of product distribution heatmap\n",
    "    '''\n",
    "\n",
    "    rt = info.RTavg\n",
    "    sim = info.similarity\n",
    "\n",
    "    # determine the figure size based on the numbers of columns and rows\n",
    "    w = 0.8658 + 0.17*len(heatmap.columns) + 1.9\n",
    "    h = 0.5941 + 0.17*len(heatmap)\n",
    "    fig, ax = plt.subplots(figsize=(w,h))\n",
    "\n",
    "    sns.heatmap(round(heatmap*100),\n",
    "                cmap = 'YlGnBu',\n",
    "                linecolor = 'lightgray',\n",
    "                cbar = False,\n",
    "                fmt = 'g',\n",
    "                vmin = 0,\n",
    "                vmax = 20,\n",
    "                xticklabels = heatmap.columns,\n",
    "                yticklabels = info.index,\n",
    "                annot = annot, # display % or not\n",
    "                square = True, # force each cell to be square\n",
    "                linewidths = 0.8,\n",
    "                )\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    ax.set_xticklabels(ax.get_xmajorticklabels(),rotation = 270)\n",
    "    ax.set_yticklabels(ax.get_ymajorticklabels(), va = 'center')\n",
    "    \n",
    "    for i in range(len(info.index)):\n",
    "        x_offset = len(heatmap.columns)\n",
    "        ax.text(x_offset + 0.25, i+0.5, info.index[i], va = 'center', ha = 'left')\n",
    "        ax.text(x_offset + 8, i+0.5, \"{:.3f}\".format(rt[i]), va = 'center', ha = 'right')\n",
    "        ax.text(x_offset + 11, i+0.5, \"{:.3f}\".format(sim[i]), va = 'center', ha = 'center')\n",
    "        \n",
    "    ax.text(x_offset + 2, -0.35, 'ID', va = 'center', ha = 'center')\n",
    "    ax.text(x_offset + 7.25, -0.35, 'RT (min)', va = 'center', ha = 'center')\n",
    "    ax.text(x_offset + 11, -0.35, 'similarity', va = 'center', ha = 'center')\n",
    "\n",
    "    for _, spine in ax.spines.items():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_color('black')\n",
    "\n",
    "    fig.savefig(f'{out_path}{core_id}_heatmap.pdf', dpi = 300, bbox_inches = 'tight')\n",
    "    print(f'...plotting finished, the heatmap saved to {out_path}{core_id}_heatmap.pdf')\n",
    "    \n",
    "\n",
    "def plot_pdt_ms(core_id, core_ms, df_peak, out_path):\n",
    "    '''\n",
    "    core_id: str\n",
    "    core_ms: DataFrame, mass spec of the core\n",
    "    df_peak: DataFrame\n",
    "    out_path: str\n",
    "    \n",
    "    output:\n",
    "    a pdf file with all product vs core mass spectra\n",
    "    '''\n",
    "    plot_color = ['black','red']\n",
    "    fig, axs = plt.subplots(math.ceil(len(df_peak)/3), 3,\n",
    "                            figsize = (6.7, 1.6*math.ceil(len(df_peak)/3)),\n",
    "                            constrained_layout=False,\n",
    "                            )\n",
    "    j = 0\n",
    "    for peak_id, val in df_peak.iterrows():\n",
    "        \n",
    "        ax = axs.flat[j]\n",
    "        peak_ms = pd.DataFrame({'mass': val.msmax.mass_list, 'counts': val.msmax.intensity_list})\n",
    "        \n",
    "        print(f'plotting {peak_id}...')\n",
    "        for i, raw_data in enumerate([peak_ms, core_ms]):\n",
    "            raw_data.counts = raw_data.counts/max(raw_data.counts)*100\n",
    "            raw_data.loc[raw_data.counts < max(raw_data.counts)* 0.01, 'counts'] = 0\n",
    "\n",
    "            ax.bar(raw_data.mass,\n",
    "                        raw_data.counts * (-2*i + 1),    # (-2*i + 1) flips the compare_ms\n",
    "                        color = plot_color[i], # i*style + i to select colors \n",
    "                        linewidth = 0,\n",
    "                        width = 1\n",
    "                        )\n",
    "\n",
    "            # only conisder ions > 10 for labeling\n",
    "            top_ions = raw_data.loc[raw_data.counts > 10].sort_values('counts', ascending = False)\n",
    "\n",
    "            # use prevent label overlapping with borders\n",
    "            blocks = [(355, 360),\n",
    "                      (0, 45)]\n",
    "            # from the highest ions,\n",
    "            # if the ion resides outside the blocks, label the ions\n",
    "            # 'block' a range of mass that has no space for text labels\n",
    "            # repeat until go through all the top_ions\n",
    "\n",
    "            counts = 0\n",
    "            add = True\n",
    "            for ind,ion in top_ions.iterrows():\n",
    "                if counts <= 5:         # label at most 5 ions\n",
    "                    if any(lower <= ion.mass - 10 <= upper for (lower, upper) in blocks):\n",
    "                        add = False\n",
    "                    elif any(lower <= ion.mass + 10 <= upper for (lower, upper) in blocks):\n",
    "                        add = False                    \n",
    "                    else:\n",
    "                        ax.text(ion.mass,\n",
    "                                (ion.counts + 0.04)*(-2*i + 1) - 35*i,\n",
    "                                # 0.04 to avoid ion label overlaps with the signals\n",
    "                                # (-2*i + 1) to flip compare_ms\n",
    "                                # - 35*i to ship ion labels in compare_ms down\n",
    "                                str(int(ion.mass)),\n",
    "                                ha='center',\n",
    "                                va='bottom',\n",
    "                                color = plot_color[i]\n",
    "                                )\n",
    "                        counts += 1\n",
    "\n",
    "                    blocks.append((int(ion.mass - 10),\n",
    "                                    int(ion.mass + 10)))\n",
    "\n",
    "        ax.axhline(color = 'black')\n",
    "        ax.set_ylim(-140,140)\n",
    "        ax.set_xlim(40,360)\n",
    "        ax.set_xticks([50,100,150,200,250,300,350])\n",
    "        ax.set_yticks([100,50,0,-50,-100])\n",
    "        ax.set_yticklabels([])\n",
    "        if j % 3 == 0:\n",
    "            ax.set_yticklabels([str(abs(x)) for x in ax.get_yticks()])\n",
    "            \n",
    "        ax.set_title(peak_id, loc = 'left')\n",
    "        \n",
    "        j += 1\n",
    "        \n",
    "    fig.supxlabel('$m/z$')\n",
    "    fig.supylabel('Ion counts (%)')\n",
    "    \n",
    "    # hide no mass spec subplots\n",
    "    if len(df_peak)%3 == 1:\n",
    "        axs.flat[-1].axis('off')\n",
    "        axs.flat[-2].axis('off')\n",
    "    elif len(df_peak)%3 == 2:\n",
    "        axs.flat[-1].axis('off')\n",
    "\n",
    "    fig.savefig(f'{out_path}{core_id}_product_ms.pdf', dpi = 300, bbox_inches = 'tight')\n",
    "    print(f'...plotting finished, mass spectra saved to {out_path}{core_id}_product_ms.pdf')\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "def finalize_report(core_id, int_path, rev_path, out_path, reorder = False):\n",
    "    '''\n",
    "    core_id: str\n",
    "    int_path: str, the path with all analyzed files\n",
    "    rev_path: str, the path has 'anlys_rev.csv'\n",
    "    out_path: str, the path for output files\n",
    "    reorder: bool, rename product id or not\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    df_peak = pd.read_pickle(f'{int_path}peak_nist.df')\n",
    "    final = pd.read_csv(f'{rev_path}anlys_rev.csv')\n",
    "    final_ratio = pd.read_csv(f'{int_path}ratio.csv', index_col = 'id')\n",
    "    \n",
    "    \n",
    "    core = df_peak.loc[df_peak['core'] == True].iloc[0]['msmax']\n",
    "    core_ms = pd.DataFrame({'mass': core.mass_list, 'counts': core.intensity_list})\n",
    "    \n",
    "    # select product labeled TRUE (considered as product)\n",
    "    final = final.loc[final['product']].sort_values(by = 'id')    \n",
    "    \n",
    "    final_ratio.columns = final_ratio.columns.str.replace(fr'{core_id}_','')\n",
    "    final_ratio = final_ratio.loc[final.id]\n",
    "    final_ratio = final_ratio.dropna(axis=1, how='all')\n",
    "    \n",
    "    final = final.set_index('id')\n",
    "    df_peak = df_peak.set_index('id')\n",
    "    df_peak = df_peak.loc[final.index]\n",
    "    \n",
    "    \n",
    "    if reorder:\n",
    "        new_index = [f'{core_id}_{str(i+1).zfill(2)}' for i in range(len(final))]\n",
    "        final.index = new_index\n",
    "        final_ratio.index = new_index\n",
    "        df_peak.index = new_index\n",
    "\n",
    "    final_ratio.to_csv(f'{out_path}{core_id}_heatmap.csv')\n",
    "    print(f'write {out_path}{core_id}_heatmap.csv')\n",
    "    \n",
    "    final[['RTavg', 'similarity']].to_csv(f'{out_path}{core_id}_pdt_info.csv')\n",
    "    print(f'write {out_path}{core_id}_pdt_info.csv')\n",
    "\n",
    "    print('start plotting the heatmap')\n",
    "    plot_heatmap(core_id = core_id,\n",
    "                 heatmap = final_ratio,\n",
    "                 info = final,\n",
    "                 out_path = out_path,\n",
    "                 annot = False)\n",
    "    \n",
    "    print('start plotting product mass spectra')\n",
    "    plot_pdt_ms(core_id = core_id,\n",
    "                core_ms = core_ms,\n",
    "                df_peak = df_peak,\n",
    "                out_path = out_path\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1. Initial data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Read CDF files\n",
    "* Save TICs for all experiments as csv files in ./tic/\n",
    "* Smooth chromatograms and peak picking (generate a list of peak objects for each experiment)\n",
    "* Save the peak objects as expr files in ./experiment/ for later use\n",
    "* Create peak_info.csv (# peaks for each chromatogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_data_process(core_id = core_id,            ### LRD to be analyzed\n",
    "                 cdf_path = cdf_path,          ### cdf file directory\n",
    "                 ref_rt = '4.78m',             ### rt of structural element in minutes\n",
    "                 msrange = [53, 350],          ### m/z range\n",
    "                 ignorems = (207, 281),        ### background ions\n",
    "                 minnoise = 500,               ### signal threshold \n",
    "                 rtrange = ['5m', '12.5m'],    ### retention time range (minutes)\n",
    "                 test = False,                 ### test run 5 files\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2. Peak alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Alignment: do both between experiments and between alkane standards, generating two alignment objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_align = alignment('./experiment', Dw = 0.5, Gw = 0.3)\n",
    "std_align = alignment('./experiment', type = 'std', Dw = 0.5, Gw = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extract information (rt, area, mass spectrum...) for each unique peak after alignment\n",
    "* Identify core: use the largest peak in no-CYP experiment\n",
    "* Compute similarity score and automatically identify potential product peaks\n",
    "* Output results\n",
    "    * ```anlys/peak.df```, DataFrame containing all analyzed information, mass spectrum objects, and peak objects\n",
    "    * ```anlys/nist_temp.df```, DataFrame to be filled with nist search results\n",
    "    * ```anlys/anlys.csv, area.csv, ratio.csv, rt.csv, std.csv```, area, ratio, or rt for each peak from all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can skip if alignment is run in the same session\n",
    "with open('./align/exp_align.aln', 'rb') as datain:\n",
    "    global_align = pickle.load(datain)\n",
    "with open('./align/std_align.aln', 'rb') as datain:\n",
    "    std_align = pickle.load(datain)\n",
    "\n",
    "df_peak = peak_analysis(global_align,           ### alignment objects for all exp\n",
    "                        std_align,              ### alignment objects for alkane standards\n",
    "                        core = core_id,         ### LRD core\n",
    "                        retention_index = 1900, ### first alkane index in the standard\n",
    "                        signal_w = 0.51,        ### weights for computing similarity\n",
    "                        mz_w = 1.1,             ### weights for computing similarity\n",
    "                        ion_threshold = 50000,  ### minimum ion counts for products\n",
    "                        similar_threshold = 0.5,### minimum similarity for products\n",
    "                        occur_threshold = 15,   ### minimum occurrence for common metabolites\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3. NIST search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NIST search for all unique peaks and save first three hits for each peak\n",
    "* Search the hit from Pubchem database to get standardized info\n",
    "* Update and save results every 5 search (search can sometimes crush)\n",
    "* Repeat if necessary (starts from the last saving point)\n",
    "* Save and output results\n",
    "    * ```anlys/peak_nist.df```, DataFrame containing all analyzed information, mass spectrum objects, peak objects, search results\n",
    "    * ```anlys/nist_temp.df```, DataFrame filled with NIST search results\n",
    "    * ```anlys/anlys_nist.csv```, anlys.csv plus NIST search results (the top hit)\n",
    "    * ```anlys_rev.csv```, same as anlys_nist.csv, manually modify this file if assignment needs to be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search = pyms_nist_search.Engine(nist_path,\n",
    "                                 pyms_nist_search.NISTMS_MAIN_LIB,\n",
    "                                 './')\n",
    "update_nist(path = 'anlys/nist_temp.df', search = search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 4. Generate a pdf report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generate a human-readable pdf report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "build_pdf_report(core_id,\n",
    "                 int_path = './anlys/',     ### path for storing all intermediate files\n",
    "                 tic_path = './tic/',       ### path for storing all tic files\n",
    "                 out_path = './'            ### path for output\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 5. Finalize product selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Recommend to go through the pdf report and modify the automatic product assignment\n",
    "* Modify the \"product\" column in analys_rev.csv to True/False\n",
    "* Generate a product distribution heatmap and stacked peak vs core mass spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalize_report(core_id = 'LRD01',\n",
    "                int_path = './anlys/',     ### path for storing all intermediate files\n",
    "                rev_path = './',           ### path for anlys_rev.csv\n",
    "                out_path = './',           ### path for output\n",
    "                reorder = True)            ### Rename peak id or not (only give id to products)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
